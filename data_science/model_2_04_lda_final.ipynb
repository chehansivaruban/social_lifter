{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPPN7HfF8tJgdsXGY3YEDqB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install pyLDAvis"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPGNLgGzKF4y","executionInfo":{"status":"ok","timestamp":1682426599918,"user_tz":-330,"elapsed":31974,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"ac17866c-c9f3-44a4-8d2f-fa28ecb1a6b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyLDAvis\n","  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy>=1.24.2\n","  Downloading numpy-1.24.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numexpr in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (2.8.4)\n","Collecting funcy\n","  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (67.7.1)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.2.2)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (4.3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (3.1.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.10.1)\n","Collecting pandas>=2.0.0\n","  Downloading pandas-2.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.2.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=2.0.0->pyLDAvis) (2022.7.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.1.0)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim->pyLDAvis) (6.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->pyLDAvis) (2.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n","Installing collected packages: funcy, numpy, pandas, pyLDAvis\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.22.4\n","    Uninstalling numpy-1.22.4:\n","      Successfully uninstalled numpy-1.22.4\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.5.3\n","    Uninstalling pandas-1.5.3:\n","      Successfully uninstalled pandas-1.5.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\n","numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\n","google-colab 1.0.0 requires pandas~=1.5.3, but you have pandas 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed funcy-2.0 numpy-1.24.3 pandas-2.0.1 pyLDAvis-3.4.1\n"]}]},{"cell_type":"markdown","source":["#Import libraries"],"metadata":{"id":"DQ-FGapoizd_"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eJMA1asLIqfL","executionInfo":{"status":"ok","timestamp":1682786437572,"user_tz":-330,"elapsed":38233,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"dab5f5f0-f6a0-4932-bf71-580b7e552d6d"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Gensim\n","import gensim\n","import gensim.corpora as corpora\n","from gensim.utils import simple_preprocess\n","from gensim.models import CoherenceModel\n","from google.colab import drive\n","\n","import numpy as np\n","import pandas as pd\n","import nltk\n","nltk.download('stopwords')\n","#spacy\n","import spacy\n","from nltk.corpus import stopwords\n","\n","# #vis\n","# import pyLDAvis\n","# import pyLDAvis.gensim\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#import the data set with sentiments added"],"metadata":{"id":"dcwVAFK_i4hW"}},{"cell_type":"code","source":["def bad_line(x):\n","  print(x)\n","  return None\n","\n","df_data = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/year4/fyp_repo/social_lifter/data_science/final/data_sentiment_final.csv', on_bad_lines=bad_line, engine='python')\n","print(df_data.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8y-tMkiOKQlX","executionInfo":{"status":"ok","timestamp":1682442854240,"user_tz":-330,"elapsed":58564,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"6a655150-3aeb-4e35-a0ed-be188b8e2989"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1150394, 26)\n"]}]},{"cell_type":"code","source":["stopwords = stopwords.words(\"english\")\n","print(len(stopwords))"],"metadata":{"id":"9Jcp1JjsLsOC","executionInfo":{"status":"ok","timestamp":1682771175139,"user_tz":-330,"elapsed":8,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b3561a20-b707-46c9-d3e6-964f7d3b95c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["179\n"]}]},{"cell_type":"code","source":["data = df_data[\"clean_tweet\"]\n","df_data['clean_tweet'] = df_data['clean_tweet'].astype(str)\n","print (data[0][0:90])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TZYoN2A8MN1t","executionInfo":{"status":"ok","timestamp":1682442854241,"user_tz":-330,"elapsed":25,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"604c119c-1072-41fd-a3aa-f66736cbfd25"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["apples amp oranges eddies guitar riffs are the songs daves riffs serve the songs amazingly\n"]}]},{"cell_type":"code","source":["def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n","    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n","    texts_out = []\n","    for text in texts:\n","        doc = nlp(text)\n","        new_text = []\n","        for token in doc:\n","            if token.pos_ in allowed_postags:\n","                new_text.append(token.lemma_)\n","        final = \" \".join(new_text)\n","        texts_out.append(final)\n","    return (texts_out)\n"],"metadata":{"id":"NcAuCZDeM23N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["['clean_tweet'] = df_data['clean_tweet'].astype(str)\n","split_data = np.array_split(df_data, 10)"],"metadata":{"id":"0VFFbtywiRYn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["split_data[0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ncjfbME9WMRx","executionInfo":{"status":"ok","timestamp":1682442856723,"user_tz":-330,"elapsed":13,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"d87135fe-619b-4e5d-9168-3f218acc7148"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(115040, 26)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["def gen_words(texts):\n","    final = []\n","    for text in texts:\n","        new = gensim.utils.simple_preprocess(text, deacc=True)\n","        final.append(new)\n","    return (final)\n","\n","df_data['clean_tweet'] = lemmatization(df_data['clean_tweet'])\n","df_data['clean_tweet'] = gen_words(df_data['clean_tweet']"],"metadata":{"id":"zKovEXJsQar8","executionInfo":{"status":"error","timestamp":1682771181768,"user_tz":-330,"elapsed":7,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"colab":{"base_uri":"https://localhost:8080/","height":130},"outputId":"847651fe-6b93-4848-8f12-d7aa72b9c18b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-8ac61b8110df>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    df_data['clean_tweet'] = gen_words(df_data['clean_tweet']\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"]}]},{"cell_type":"code","source":["for i in range(0, 10):\n","    print(i)\n","    split_data[i]['clean_tweet'] = lemmatization(split_data[i]['clean_tweet'])\n","    split_data[i]['clean_tweet'] = gen_words(split_data[i]['clean_tweet'])\n","    split_data[i].to_csv(f'data_sentiment_final_lemmatized_single_topic_{i}.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MwRe18klmVpj","executionInfo":{"status":"ok","timestamp":1682448952314,"user_tz":-330,"elapsed":6095599,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"100f8e6f-c75e-4bfc-f206-e9dbe0913f0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n"]}]},{"cell_type":"markdown","source":["#import the data set after lematizing"],"metadata":{"id":"brpqoKbwjEKH"}},{"cell_type":"code","source":["def bad_line(x):\n","  print(x)\n","  return None\n","df_data_0 = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/year4/fyp_repo/social_lifter/data_science/data_set/final_data/data_sentiment_final_lemmatized_single_topic_0.csv', on_bad_lines=bad_line, engine='python')\n","df_data_1 = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/year4/fyp_repo/social_lifter/data_science/data_set/final_data/data_sentiment_final_lemmatized_single_topic_1.csv', on_bad_lines=bad_line, engine='python')\n"],"metadata":{"id":"_k0OYLnCIQk_","executionInfo":{"status":"ok","timestamp":1682786450256,"user_tz":-330,"elapsed":12688,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["df_data_2 = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/year4/fyp_repo/social_lifter/data_science/data_set/final_data/data_sentiment_final_lemmatized_single_topic_2.csv', on_bad_lines=bad_line, engine='python')\n","df_data_3 = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/year4/fyp_repo/social_lifter/data_science/data_set/final_data/data_sentiment_final_lemmatized_single_topic_3.csv', on_bad_lines=bad_line, engine='python')\n","df_data_4 = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/year4/fyp_repo/social_lifter/data_science/data_set/final_data/data_sentiment_final_lemmatized_single_topic_4.csv', on_bad_lines=bad_line, engine='python')\n","df_data_5 = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/year4/fyp_repo/social_lifter/data_science/data_set/final_data/data_sentiment_final_lemmatized_single_topic_5.csv', on_bad_lines=bad_line, engine='python')\n","# df_data_6 = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/year4/fyp_repo/social_lifter/data_science/data_set/final_data/data_sentiment_final_lemmatized_single_topic_6.csv', on_bad_lines=bad_line, engine='python')\n","# df_data_7 = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/year4/fyp_repo/social_lifter/data_science/data_set/final_data/data_sentiment_final_lemmatized_single_topic_7.csv', on_bad_lines=bad_line, engine='python')\n","# df_data_8 = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/year4/fyp_repo/social_lifter/data_science/data_set/final_data/data_sentiment_final_lemmatized_single_topic_8.csv', on_bad_lines=bad_line, engine='python')\n","# df_data_9 = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/year4/fyp_repo/social_lifter/data_science/data_set/final_data/data_sentiment_final_lemmatized_single_topic_9.csv', on_bad_lines=bad_line, engine='python')\n"],"metadata":{"id":"LBduNjLXjS5e","executionInfo":{"status":"ok","timestamp":1682786465250,"user_tz":-330,"elapsed":15013,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["df_data_0.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q1zEBsHtVYjd","executionInfo":{"status":"ok","timestamp":1682786465254,"user_tz":-330,"elapsed":37,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"c31d658c-15bb-4e00-9f0c-548996b73f0e"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(115040, 26)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# Concatenate all dataframes into one\n","df_data = pd.concat([df_data_0, df_data_1, df_data_2, df_data_3, df_data_4, df_data_5])\n","\n","# Reset the index of the concatenated dataframe\n","df_data = df_data.reset_index(drop=True)"],"metadata":{"id":"FckYxjp2SkyG","executionInfo":{"status":"ok","timestamp":1682786465754,"user_tz":-330,"elapsed":534,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["df_data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"37iuAnGkUmut","executionInfo":{"status":"ok","timestamp":1682786465756,"user_tz":-330,"elapsed":9,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"bebe10fa-317f-4f63-f835-074283161c10"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(690238, 26)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# Create a dictionary of lemmatized words\n","df_data['clean_tweet'] = df_data['clean_tweet'].astype(str)\n","word_dict = corpora.Dictionary(df_data['clean_tweet'].apply(lambda x: x.split()))\n","\n","# Create a bag of words corpus\n","corpus = [word_dict.doc2bow(doc.split()) for doc in df_data['clean_tweet']]\n"],"metadata":{"id":"x9Mx3g-kLyTW","executionInfo":{"status":"ok","timestamp":1682786493560,"user_tz":-330,"elapsed":27811,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Train LDA model\n","lda_model = gensim.models.LdaModel(corpus=corpus,\n","                                   id2word=word_dict,\n","                                   num_topics=250, \n","                                   random_state=42,\n","                                   passes=10,\n","                                   alpha='auto',\n","                                   per_word_topics=True)"],"metadata":{"id":"nANIizMsMi5M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","# Save LDA model as a pickle file\n","with open('lda_model.pickle', 'wb') as f:\n","    pickle.dump(lda_model, f)"],"metadata":{"id":"zd8BeRZENUhA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save LDA model\n","lda_model.save('lda_model')"],"metadata":{"id":"FUOL6ClabYZr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_data.to_csv('data_sentiment_final_lemmatized_single_topic_detected.csv', index=False)"],"metadata":{"id":"xZ1lG1v8NYwt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a dictionary of lemmatized words\n","df_data['clean_tweet'] = df_data['clean_tweet'].astype(str)\n","word_dict = corpora.Dictionary(df_data['clean_tweet'].apply(lambda x: x.split()))\n","\n","# Create a bag of words corpus\n","corpus = [word_dict.doc2bow(doc.split()) for doc in df_data['clean_tweet']]\n","\n","# Find the most probable topic for each document\n","topics = []\n","for doc in corpus:\n","    doc_str = ' '.join([word_dict[id] for id, freq in doc])\n","    bow = word_dict.doc2bow(doc_str.split())\n","    topic_dist = lda_model.get_document_topics(bow, minimum_probability=0.0)\n","    sorted_topics = sorted(topic_dist, key=lambda x: x[1], reverse=True)\n","    most_probable_topic = sorted_topics[0][0]\n","    topics.append(most_probable_topic)\n","\n","# Save the topics in a new column in df_data\n","df_data['topics'] = topics"],"metadata":{"id":"6UUYklGccXOL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_data.head()"],"metadata":{"id":"OXxfWN0yetZ9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#usage"],"metadata":{"id":"-IXqok_OlCRT"}},{"cell_type":"code","source":["df_data.to_csv('preprocessed_data.csv', index=False)"],"metadata":{"id":"TCAGjnal6Jno"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","import numpy as np\n","import gensim\n","\n","# Load the LDA model\n","with open('/content/drive/Othercomputers/My Laptop (1)/year4/fyp_repo/social_lifter/data_science/final_models/lda_model/lda_model.pickle', 'rb') as f:\n","    lda_model = pickle.load(f)\n","\n","# Load the id2word dictionary\n","id2word = gensim.corpora.Dictionary.load('/content/drive/Othercomputers/My Laptop (1)/year4/fyp_repo/social_lifter/data_science/final_models/lda_model/lda_model.id2word')\n","\n","# Load the state\n","with open('/content/drive/Othercomputers/My Laptop (1)/year4/fyp_repo/social_lifter/data_science/final_models/lda_model/lda_model.state', 'rb') as f:\n","    lda_model.state = pickle.load(f)\n","\n","# Load the expElogbeta array\n","lda_model.expElogbeta = np.load('/content/drive/Othercomputers/My Laptop (1)/year4/fyp_repo/social_lifter/data_science/final_models/lda_model/lda_model.expElogbeta.npy')\n","\n","# Define the sentence to classify\n","sentence = \"Perplexity captures how surprised a model is of new data it has not seen before, and is measured as the normalized log-likelihood of a held-out test set.\"\n","\n","# Prepare the sentence for classification\n","bow = id2word.doc2bow(sentence.lower().split())\n","\n","# Classify the sentence\n","topic_dist = lda_model.get_document_topics(bow, minimum_probability=0.0)\n","topic = sorted(topic_dist, key=lambda x: x[1], reverse=True)[0][0]\n","\n","# Print the topic\n","print(\"The topic of the sentence is:\", topic)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jcBxuHCVk9tU","executionInfo":{"status":"ok","timestamp":1682487936429,"user_tz":-330,"elapsed":597,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"8937bbd9-76be-4630-a02b-0fbbe899f496"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The topic of the sentence is: 89\n"]}]},{"cell_type":"code","source":["topics = []\n","for doc in corpus:\n","    doc_str = ' '.join([word_dict[id] for id, freq in doc])\n","    bow = id2word.doc2bow(doc_str.split())\n","    topic_dist = lda_model.get_document_topics(bow, minimum_probability=0.0)\n","    sorted_topics = sorted(topic_dist, key=lambda x: x[1], reverse=True)\n","    most_probable_topic = sorted_topics[0][0]\n","    topics.append(most_probable_topic)\n","\n","# Save the topics in a new column in df_data\n","df_data['topics'] = topics"],"metadata":{"id":"pxCAOyHWFHT0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_data.to_csv('data_sentiment_final_lemmatized_single_topic_detected.csv', index=False)"],"metadata":{"id":"CFCRjiUFJ3E_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim.models import CoherenceModel\n","\n","# Create a coherence model\n","coherence_model = CoherenceModel(lda_model, corpus=corpus, dictionary=word_dict, coherence='c_v', texts=df_data['clean_tweet'])\n","\n","# Calculate the coherence score\n","coherence_score = coherence_model.get_coherence()\n","\n","print('Coherence score:', coherence_score)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KO-aNl7zNO7N","executionInfo":{"status":"ok","timestamp":1682493650136,"user_tz":-330,"elapsed":241469,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"59f1b824-f9ef-4b17-a4f3-4c8c6a4ef1e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/gensim/topic_coherence/direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in double_scalars\n","  m_lr_i = np.log(numerator / denominator)\n","/usr/local/lib/python3.9/dist-packages/gensim/topic_coherence/indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in double_scalars\n","  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n"]},{"output_type":"stream","name":"stdout","text":["Coherence score: nan\n"]}]}]}