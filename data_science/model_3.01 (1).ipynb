{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNeApcVGpnw6mSwxc1I0wUG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["#Importing libaries"],"metadata":{"id":"lfdfW--rvkOR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"t46Pr_ovvKW7"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from scipy.sparse import hstack"]},{"cell_type":"markdown","source":["#Mounting Google Drive"],"metadata":{"id":"JLpljTK7voms"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IPO19I6uvhBw","executionInfo":{"status":"ok","timestamp":1681452705633,"user_tz":-330,"elapsed":55889,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"8b724958-9cf8-458d-cd17-331284427070"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["def bad_line(x):\n","    print(x)\n","    return None\n","\n","df_data = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/year4/final_project/data_science/data_set/topic_data_1.csv',  on_bad_lines=bad_line, engine='python')\n","\n","\n","df_data.dtypes"],"metadata":{"id":"UWRyydwkv_wP","executionInfo":{"status":"ok","timestamp":1681452713591,"user_tz":-330,"elapsed":7963,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"6969eccd-2241-478f-94d4-eacc9595f047","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Tweet                       object\n","Date                        object\n","time                        object\n","Day of week                 object\n","Cashtags                    object\n","Hashtags                    object\n","Language                    object\n","Location                      bool\n","Mentioned_users               bool\n","Followers                  float64\n","Following                  float64\n","User_created_date           object\n","Listed_count               float64\n","Favourite_count            float64\n","Tweet_count                float64\n","Verified                      bool\n","Average_favourite_count    float64\n","account_age                float64\n","Likes                      float64\n","Comments                   float64\n","Retweets                   float64\n","Views                      float64\n","clean_tweet                 object\n","subjectivity               float64\n","polarity                   float64\n","sentiment                   object\n","topics                       int64\n","key_words                   object\n","dtype: object"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["df_data_linear = df_data.copy()"],"metadata":{"id":"TamJZlcvNIBF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert \"Cashtags\" column to boolean\n","df_data_linear['Cashtags'] = np.where(df_data_linear['Cashtags'].notnull(), True, False)\n","\n","# Convert \"Hashtags\" column to boolean\n","df_data_linear['Hashtags'] = np.where(df_data_linear['Hashtags'].notnull(), True, False)\n"],"metadata":{"id":"LvW4xVKFv_4h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop the columns we won't use for prediction\n","df_data_linear.drop(['Tweet', 'Date', 'Listed_count', 'Favourite_count', 'Tweet_count', 'User_created_date', 'Views'], axis=1, inplace=True)\n"],"metadata":{"id":"6A-2YhxLwAUK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Cleaning the data for Linear regression"],"metadata":{"id":"451H2ypNI3Zv"}},{"cell_type":"markdown","source":["\n","# Convert 'time' column to numerical"],"metadata":{"id":"fSXj9FkhI8zu"}},{"cell_type":"code","source":["df_data_linear['time'] = pd.to_datetime(df_data_linear['time']).dt.hour"],"metadata":{"id":"VLLaXpzkwAdA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Encode categorical variables"],"metadata":{"id":"CaNDF1GyJeh1"}},{"cell_type":"code","source":["cat_cols = ['Day of week', 'Language', 'sentiment', 'topics', 'key_words', 'time']\n","for col in cat_cols:\n","    label_encoder = LabelEncoder()\n","    df_data_linear[col] = label_encoder.fit_transform(df_data_linear[col])\n"],"metadata":{"id":"lxA2FajrQOUC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create feature matrix with one-hot encoded categorical variables and token counts\n","vectorizer = CountVectorizer(stop_words='english')\n","vectorizer.fit(df_data_linear['clean_tweet'])\n","X_text = vectorizer.transform(df_data_linear['clean_tweet'])\n","cat_cols = ['Cashtags', 'Hashtags', 'Location', 'Mentioned_users', 'Verified', 'Day of week', 'Language', 'sentiment', 'topics', 'key_words', 'time']\n","cat_transformer = OneHotEncoder()\n","cat_transformer.fit(df_data_linear[cat_cols])\n","X_cat = cat_transformer.transform(df_data_linear[cat_cols])\n","X_num = StandardScaler().fit_transform(df_data_linear.select_dtypes(include=np.number))\n","X = hstack([X_cat, X_text, X_num])\n","\n"],"metadata":{"id":"4Rzs4idPQQ2j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Create target vectors\n","y_likes = df_data_linear['Likes']\n","y_comments = df_data_linear['Comments']\n","y_retweets = df_data_linear['Retweets']\n"],"metadata":{"id":"IH-CWMk1QUMT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data into training and testing sets\n","X_train, X_test, y_likes_train, y_likes_test, y_comments_train, y_comments_test, y_retweets_train, y_retweets_test = train_test_split(X, y_likes, y_comments, y_retweets, test_size=0.2, random_state=42)"],"metadata":{"id":"_arv9JacQWpP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score\n","# Create a Linear Regression model for likes\n","lr_likes = LinearRegression()\n","\n","# Fit the model on the training data for likes\n","lr_likes.fit(X_train, y_likes_train)\n","\n","# Predict on the testing data for likes\n","y_likes_pred = lr_likes.predict(X_test)\n","\n","# Evaluate the model using mean squared error and R-squared for likes\n","mse_likes = mean_squared_error(y_likes_test, y_likes_pred)\n","r2_likes = r2_score(y_likes_test, y_likes_pred)\n","\n","print('Mean squared error (Likes):', mse_likes)\n","print('R-squared (Likes):', r2_likes)\n","\n","# Create a Linear Regression model for comments\n","lr_comments = LinearRegression()\n","\n","# Fit the model on the training data for comments\n","lr_comments.fit(X_train, y_comments_train)\n","\n","# Predict on the testing data for comments\n","y_comments_pred = lr_comments.predict(X_test)\n","\n","# Evaluate the model using mean squared error and R-squared for comments\n","mse_comments = mean_squared_error(y_comments_test, y_comments_pred)\n","r2_comments = r2_score(y_comments_test, y_comments_pred)\n","\n","print('Mean squared error (Comments):', mse_comments)\n","print('R-squared (Comments):', r2_comments)\n","\n","# Create a Linear Regression model for retweets\n","lr_retweets = LinearRegression()\n","\n","# Fit the model on the training data for retweets\n","lr_retweets.fit(X_train, y_retweets_train)\n","\n","# Predict on the testing data for retweets\n","y_retweets_pred = lr_retweets.predict(X_test)\n","\n","# Evaluate the model using mean squared error and R-squared for retweets\n","mse_retweets = mean_squared_error(y_retweets_test, y_retweets_pred)\n","r2_retweets = r2_score(y_retweets_test, y_retweets_pred)\n","\n","print('Mean squared error (Retweets):', mse_retweets)\n","print('R-squared (Retweets):', r2_retweets)"],"metadata":{"id":"pt6Y47VUQXqu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681452745349,"user_tz":-330,"elapsed":19012,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"ca203475-2fdc-45c9-c5e1-c39f66f78f4e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean squared error (Likes): 1.775328852541558e-05\n","R-squared (Likes): 0.9999999991289109\n","Mean squared error (Comments): 7.281810200888075e-08\n","R-squared (Comments): 0.9999999997215915\n","Mean squared error (Retweets): 1.5945309175491014e-06\n","R-squared (Retweets): 0.9999999995967612\n"]}]},{"cell_type":"code","source":["import joblib\n","\n","# Save the trained model for Likes\n","joblib.dump(lr_likes, 'lr_likes_model.pkl')\n","\n","\n","# Save the trained model for Comments\n","joblib.dump(lr_comments, 'lr_comments_model.pkl')\n","\n","# Save the trained model for Retweets\n","joblib.dump(lr_retweets, 'lr_retweets_model.pkl')"],"metadata":{"id":"--f6AKkxQXoN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681453401297,"user_tz":-330,"elapsed":7,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"1cdb5cae-19c9-4ee8-841b-b37414f4ee41"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['lr_retweets_model.pkl']"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeRegressor\n","\n","# Create a Decision Tree model for likes\n","dt_likes = DecisionTreeRegressor()\n","\n","# Fit the model on the training data for likes\n","dt_likes.fit(X_train, y_likes_train)\n","\n","# Predict on the testing data for likes\n","y_likes_pred = dt_likes.predict(X_test)\n","\n","# Evaluate the model using mean squared error and R-squared for likes\n","mse_likes = mean_squared_error(y_likes_test, y_likes_pred)\n","r2_likes = r2_score(y_likes_test, y_likes_pred)\n","\n","print('Mean squared error (Likes):', mse_likes)\n","print('R-squared (Likes):', r2_likes)\n"],"metadata":{"id":"S3bUfTrwQXjb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681453298044,"user_tz":-330,"elapsed":20851,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"0c1c6829-a7c5-472b-8c30-26ff66081103"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean squared error (Likes): 18.897371045882018\n","R-squared (Likes): 0.9990727749866369\n"]}]},{"cell_type":"code","source":["\n","# Create a Decision Tree model for comments\n","dt_comments = DecisionTreeRegressor()\n","\n","# Fit the model on the training data for comments\n","dt_comments.fit(X_train, y_comments_train)\n","\n","# Predict on the testing data for comments\n","y_comments_pred = dt_comments.predict(X_test)\n","\n","# Evaluate the model using mean squared error and R-squared for comments\n","mse_comments = mean_squared_error(y_comments_test, y_comments_pred)\n","r2_comments = r2_score(y_comments_test, y_comments_pred)\n","\n","print('Mean squared error (Comments):', mse_comments)\n","print('R-squared (Comments):', r2_comments)\n"],"metadata":{"id":"XdnA2TCtQXg2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681453345527,"user_tz":-330,"elapsed":18977,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"7c140199-4b61-42d0-8fe8-e79a97489d00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean squared error (Comments): 2.6180535765175263\n","R-squared (Comments): 0.9899902839690496\n"]}]},{"cell_type":"code","source":["\n","# Create a Decision Tree model for retweets\n","dt_retweets = DecisionTreeRegressor()\n","\n","# Fit the model on the training data for retweets\n","dt_retweets.fit(X_train, y_retweets_train)\n","\n","# Predict on the testing data for retweets\n","y_retweets_pred = dt_retweets.predict(X_test)\n","\n","# Evaluate the model using mean squared error and R-squared for retweets\n","mse_retweets = mean_squared_error(y_retweets_test, y_retweets_pred)\n","r2_retweets = r2_score(y_retweets_test, y_retweets_pred)\n","\n","print('Mean squared error (Retweets):', mse_retweets)\n","print('R-squared (Retweets):', r2_retweets)\n","\n"],"metadata":{"id":"4Ec6cAjQQXeS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681453372630,"user_tz":-330,"elapsed":17087,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"70425d54-c7dd-4002-99d5-5ff797e232df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean squared error (Retweets): 46.23774579652323\n","R-squared (Retweets): 0.9883069990393644\n"]}]},{"cell_type":"code","source":["\n","# Save the trained model for Likes\n","joblib.dump(dt_likes, 'dt_likes_model.pkl')\n","\n","\n","# Save the trained model for Comments\n","joblib.dump(dt_comments, 'dt_comments_model.pkl')\n","\n","# Save the trained model for Retweets\n","joblib.dump(dt_retweets, 'dt_retweets_model.pkl')"],"metadata":{"id":"sNCC_DhHQXcJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681453406890,"user_tz":-330,"elapsed":816,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"873e0c37-4b6f-43ff-e76e-19870361fbc8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['dt_retweets_model.pkl']"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","\n","# Create a Random Forest model for Likes\n","rf_likes = RandomForestRegressor(n_estimators=100, random_state=42)\n","\n","# Fit the model on the training data for likes\n","rf_likes.fit(X_train, y_likes_train)\n","\n","# Predict on the testing data for likes\n","y_likes_pred = rf_likes.predict(X_test)\n","\n","# Evaluate the model using mean squared error and R-squared for likes\n","mse_likes = mean_squared_error(y_likes_test, y_likes_pred)\n","r2_likes = r2_score(y_likes_test, y_likes_pred)\n","\n","print('Mean squared error (Likes):', mse_likes)\n","print('R-squared (Likes):', r2_likes)\n","\n","# Create a Random Forest model for Comments\n","rf_comments = RandomForestRegressor(n_estimators=100, random_state=42)\n","\n","# Fit the model on the training data for comments\n","rf_comments.fit(X_train, y_comments_train)\n","\n","# Predict on the testing data for comments\n","y_comments_pred = rf_comments.predict(X_test)\n","\n","# Evaluate the model using mean squared error and R-squared for comments\n","mse_comments = mean_squared_error(y_comments_test, y_comments_pred)\n","r2_comments = r2_score(y_comments_test, y_comments_pred)\n","\n","print('Mean squared error (Comments):', mse_comments)\n","print('R-squared (Comments):', r2_comments)\n","\n","# Create a Random Forest model for Retweets\n","rf_retweets = RandomForestRegressor(n_estimators=100, random_state=42)\n","\n","# Fit the model on the training data for retweets\n","rf_retweets.fit(X_train, y_retweets_train)\n","\n","# Predict on the testing data for retweets\n","y_retweets_pred = rf_retweets.predict(X_test)\n","\n","# Evaluate the model using mean squared error and R-squared for retweets\n","mse_retweets = mean_squared_error(y_retweets_test, y_retweets_pred)\n","r2_retweets = r2_score(y_retweets_test, y_retweets_pred)\n","\n","print('Mean squared error (Retweets):', mse_retweets)\n","print('R-squared (Retweets):', r2_retweets)\n"],"metadata":{"id":"Vh-i20aAQXZg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681457343128,"user_tz":-330,"elapsed":3441548,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"7b26c369-07af-40de-c06c-cee9e834b87a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean squared error (Likes): 4.1064778889997156\n","R-squared (Likes): 0.9997985101204682\n","Mean squared error (Comments): 7.33630169920205\n","R-squared (Comments): 0.9719508044506591\n","Mean squared error (Retweets): 18.673644432174424\n","R-squared (Retweets): 0.9952776473307139\n"]}]},{"cell_type":"code","source":["\n","# Save the trained model for Likes\n","joblib.dump(rf_likes, 'rf_likes_model.pkl')\n","\n","# Save the trained model for Comments\n","joblib.dump(rf_comments, 'rf_comments_model.pkl')\n","\n","# Save the trained model for Retweets\n","joblib.dump(rf_retweets, 'rf_retweets_model.pkl')"],"metadata":{"id":"Cn6VzIZ1QXWm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RCD2asV7QXUA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_encoder = LabelEncoder()\n","df_data_linear['Day of week'] = label_encoder.fit_transform(df_data_linear['Day of week'])\n","df_data_linear['Language'] = label_encoder.fit_transform(df_data_linear['Language'])\n","df_data_linear['sentiment'] = label_encoder.fit_transform(df_data_linear['sentiment'])\n","df_data_linear['topics'] = label_encoder.fit_transform(df_data_linear['topics'])\n","df_data_linear['key_words'] = label_encoder.fit_transform(df_data_linear['key_words'])\n","\n","\n","# create an instance of CountVectorizer with desired settings\n","vectorizer = CountVectorizer(stop_words='english')\n","\n","# fit the vectorizer to the clean_tweet column\n","vectorizer.fit(df_data_linear['clean_tweet'])\n","\n","# transform the clean_tweet column into a matrix of token counts\n","X_text = vectorizer.transform(df_data_linear['clean_tweet'])"],"metadata":{"id":"UCBcLgEIJfmQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# One-hot encode boolean variables"],"metadata":{"id":"0qcKZYcpKZ0n"}},{"cell_type":"code","source":["transformer = ColumnTransformer(transformers=[('OneHot', OneHotEncoder(), ['Cashtags', 'Hashtags', 'Location', 'Mentioned_users', 'Verified'])], remainder='passthrough')\n","df_data_linear = pd.DataFrame(transformer.fit_transform(df_data_linear))"],"metadata":{"id":"dAzpAKZaKaYH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# concatenate the one-hot encoded features, text features, and numerical features\n","X = np.concatenate((transformer.transform(df_data_linear), X_text.toarray(), df_data_linear[['Followers', 'Friends', 'time', 'Day of week', 'Language', 'sentiment']].values), axis=1)\n","\n","# scale the numerical features using StandardScaler\n","scaler = StandardScaler()\n","X[:, -6:] = scaler.fit_transform(X[:, -6:])\n","\n","# set the target variables (Likes, Comments, and Retweets)\n","y = df_data_linear[['Likes', 'Comments', 'Retweets']].values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"id":"Pmwu5x4xOaet","executionInfo":{"status":"error","timestamp":1681382886387,"user_tz":-330,"elapsed":373,"user":{"displayName":"Chehan Sivaruban","userId":"13443851556090488132"}},"outputId":"c604cfc7-66fc-427e-d7cb-8679fe482c27"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-935df27791d3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# concatenate the one-hot encoded features, text features, and numerical features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data_linear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_data_linear\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Followers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Friends'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Day of week'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Language'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# scale the numerical features using StandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_names\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"columns are missing: {diff}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0;31m# ndarray was used for fitting or transforming, thus we only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: columns are missing: {'time', 'clean_tweet', 'Followers', 'key_words', 'Retweets', 'Likes', 'subjectivity', 'Average_favourite_count', 'Comments', 'Day of week', 'Following', 'sentiment', 'polarity', 'Language', 'topics', 'account_age'}"]}]}]}